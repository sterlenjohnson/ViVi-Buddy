# Inference Engines & Software Tools (2025)

*Last Updated: November 25, 2025*

A comprehensive guide to the software ecosystem for running LLMs locally.

## üöÇ Inference Engines

### llama.cpp
**URL:** [github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)  
**License:** MIT  
**Best For:** CPU, Apple Silicon, NVIDIA, AMD (via ROCm/HIP)

**Pros:**
- ‚úÖ Universal compatibility (Windows/Mac/Linux)
- ‚úÖ GGUF format support (standard for local LLMs)
- ‚úÖ Active development (weekly updates)
- ‚úÖ Low VRAM overhead
- ‚úÖ Hybrid CPU+GPU inference

**Cons:**
- ‚ùå Slower prompt processing than ExLlamaV2 on NVIDIA
- ‚ùå Command-line focused (though many GUIs wrap it)

**Backend Support:**
- **CUDA** (NVIDIA)
- **Metal** (Apple)
- **ROCm/HIP** (AMD)
- **Vulkan** (Universal, slower)
- **SYCL** (Intel Arc)
- **CPU** (Always available)

**Typical Usage:**
```bash
llama-cli -m llama-3.1-70b-q4_k_m.gguf -c 8192 -ngl 99 --color
```

---

### Ollama
**URL:** [ollama.com](https://ollama.com)  
**License:** MIT  
**Best For:** Beginners, quick testing, API servers

**Pros:**
- ‚úÖ One-command model downloads (`ollama run llama3`)
- ‚úÖ Built-in model library
- ‚úÖ OpenAI-compatible API
- ‚úÖ Automatic GPU detection
- ‚úÖ Simple updates (`ollama pull`)

**Cons:**
- ‚ùå Less control over advanced settings
- ‚ùå Wraps llama.cpp (adds overhead)
- ‚ùå Limited to curated model list (though you can import custom)

**Built on:** llama.cpp (uses GGUF files internally)

**Typical Usage:**
```bash
ollama run llama3.1:70b
```

---

### LM Studio
**URL:** [lmstudio.ai](https://lmstudio.ai)  
**License:** Proprietary (Free for personal use)  
**Best For:** GUI users, Windows/Mac

**Pros:**
- ‚úÖ Beautiful GUI
- ‚úÖ Built-in model browser (Hugging Face integration)
- ‚úÖ Chat UI, server mode, and code completion
- ‚úÖ Advanced settings exposed (RoPE scaling, Flash Attention, etc.)
- ‚úÖ VRAM usage preview before loading

**Cons:**
- ‚ùå Proprietary (not open source)
- ‚ùå Slightly higher VRAM overhead than raw llama.cpp
- ‚ùå Intel Mac support is limited

**Built on:** llama.cpp

**Typical Usage:** Click, download, chat.

---

### ExLlamaV2
**URL:** [github.com/turboderp/exllamav2](https://github.com/turboderp/exllamav2)  
**License:** MIT  
**Best For:** NVIDIA GPUs (speed priority)

**Pros:**
- ‚úÖ **Fastest** NVIDIA inference (beats llama.cpp by ~30%)
- ‚úÖ EXL2 format (variable bitrate quantization)
- ‚úÖ Dynamic generator (adaptive batching)

**Cons:**
- ‚ùå NVIDIA-only (CUDA required)
- ‚ùå No GGUF support (different format)
- ‚ùå More complex setup

**Format:** EXL2 (not GGUF)

**Typical Usage:**
```bash
python -m exllamav2.server --model llama-3.1-70b-exl2-4.0bpw
```

---

### vLLM
**URL:** [github.com/vllm-project/vllm](https://github.com/vllm-project/vllm)  
**License:** Apache 2.0  
**Best For:** Production servers, high throughput

**Pros:**
- ‚úÖ Highest throughput (batching, continuous batching)
- ‚úÖ PagedAttention (efficient KV cache management)
- ‚úÖ OpenAI-compatible API
- ‚úÖ Multi-GPU support

**Cons:**
- ‚ùå High VRAM overhead (reserves memory for batching)
- ‚ùå Complex setup
- ‚ùå Overkill for single-user chat

**Best Use Case:** Serving many users simultaneously.

---

### MLX (Apple-Specific)
**URL:** [github.com/ml-explore/mlx](https://github.com/ml-explore/mlx)  
**License:** MIT  
**Best For:** Apple Silicon (M1/M2/M3/M4/M5)

**Pros:**
- ‚úÖ Native Apple framework
- ‚úÖ Optimized for Metal
- ‚úÖ Growing ecosystem

**Cons:**
- ‚ùå Apple-only
- ‚ùå Fewer pre-built models than GGUF
- ‚ùå Less mature than llama.cpp

**Note:** llama.cpp Metal backend is still faster on many models.

---

## üì± GUI Applications

### LM Studio
See above. The most polished GUI.

### GPT4All
**URL:** [gpt4all.io](https://gpt4all.io)  
**License:** MIT  
**Best For:** Privacy-focused beginners

**Pros:**
- ‚úÖ Simple, clean UI
- ‚úÖ Curated model list
- ‚úÖ Local-first

**Cons:**
- ‚ùå Limited to smaller models
- ‚ùå Less control than LM Studio

### Jan
**URL:** [jan.ai](https://jan.ai)  
**License:** AGPL-3.0  
**Best For:** Open-source GUI

**Pros:**
- ‚úÖ Fully open source
- ‚úÖ Multi-engine support

**Cons:**
- ‚ùå Newer, less polished

---

## üñ•Ô∏è Web UIs

### Text-Generation-WebUI (oobabooga)
**URL:** [github.com/oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui)  
**License:** AGPL-3.0  
**Best For:** Advanced users, extensions

**Pros:**
- ‚úÖ Highly customizable
- ‚úÖ Extension system
- ‚úÖ Multi-backend (llama.cpp, ExLlamaV2, Transformers)

**Cons:**
- ‚ùå Complex setup
- ‚ùå UI is dated

### Open WebUI (formerly Ollama WebUI)
**URL:** [github.com/open-webui/open-webui](https://github.com/open-webui/open-webui)  
**License:** MIT  
**Best For:** Self-hosted ChatGPT-like interface

**Pros:**
- ‚úÖ Modern UI (looks like ChatGPT)
- ‚úÖ Multi-user support
- ‚úÖ Works with Ollama or any OpenAI-compatible API

**Cons:**
- ‚ùå Requires Ollama/API backend

---

## ‚öôÔ∏è Which Should You Use?

### Windows User, Want GUI
‚Üí **LM Studio**

### Mac User (Apple Silicon)
‚Üí **Ollama** or **LM Studio**

### Linux User, Want Speed
‚Üí **llama.cpp** (direct) or **ExLlamaV2** (NVIDIA only)

### Want Maximum Speed (NVIDIA)
‚Üí **ExLlamaV2**

### Want Simplicity
‚Üí **Ollama**

### Serving Multiple Users
‚Üí **vLLM** or **Ollama** (API mode)

### Privacy Paranoid
‚Üí **GPT4All** (doesn't phone home)

---

## üìä Performance Comparison (Rough Estimates)

**Llama 3.1 70B Q4, RTX 4090, 8k context, CUDA**

| Engine | Prompt (tok/s) | Generation (tok/s) |
|--------|----------------|---------------------|
| **ExLlamaV2** | ~600 | ~45 |
| **llama.cpp (CUDA)** | ~450 | ~42 |
| **Ollama (llama.cpp)** | ~420 | ~40 |
| **LM Studio** | ~400 | ~38 |

**Notes:**
- Prompt speed varies wildly based on context size
- Generation speed is more consistent
- These are ballpark figures; your mileage will vary

---

## üîó Installation Quick Links

- **llama.cpp releases:** [github.com/ggerganov/llama.cpp/releases](https://github.com/ggerganov/llama.cpp/releases)
- **Ollama install:** [ollama.com/download](https://ollama.com/download)
- **LM Studio download:** [lmstudio.ai](https://lmstudio.ai)
